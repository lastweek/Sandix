menu "General arch-independent setup"

config JUMP_LABEL
       bool "Optimize very unlikely/likely branches"
       depends on HAVE_ARCH_JUMP_LABEL
       help
         This option enables a transparent branch optimization that
	 makes certain almost-always-true or almost-always-false branch
	 conditions even cheaper to execute within the kernel.

	 Certain performance-sensitive kernel code, such as trace points,
	 scheduler functionality, networking code and KVM have such
	 branches and include support for this optimization technique.

         If it is detected that the compiler has support for "asm goto",
	 the kernel will compile such branches with just a nop
	 instruction. When the condition flag is toggled to true, the
	 nop will be converted to a jump instruction to execute the
	 conditional block of instructions.

	 This technique lowers overhead and stress on the branch prediction
	 of the processor and generally makes the kernel faster. The update
	 of the condition is slower, but those are always very rare.

	 ( On 32-bit x86, the necessary options added to the compiler
	   flags may increase the size of the kernel slightly. )

config HAVE_64BIT_ALIGNED_ACCESS
	def_bool 64BIT && !HAVE_EFFICIENT_UNALIGNED_ACCESS
	help
	  Some architectures require 64 bit accesses to be 64 bit
	  aligned, which also requires structs containing 64 bit values
	  to be 64 bit aligned too. This includes some 32 bit
	  architectures which can do 64 bit accesses, as well as 64 bit
	  architectures without unaligned access.

	  This symbol should be selected by an architecture if 64 bit
	  accesses are required to be 64 bit aligned in this way even
	  though it is not a 64 bit architecture.

	  See Documentation/unaligned-memory-access.txt for more
	  information on the topic of unaligned memory accesses.

config HAVE_EFFICIENT_UNALIGNED_ACCESS
	bool
	help
	  Some architectures are unable to perform unaligned accesses
	  without the use of get_unaligned/put_unaligned. Others are
	  unable to perform such accesses efficiently (e.g. trap on
	  unaligned access and require fixing it up in the exception
	  handler.)

	  This symbol should be selected by an architecture if it can
	  perform unaligned accesses efficiently to allow different
	  code paths to be selected for these cases. Some network
	  drivers, for example, could opt to not fix up alignment
	  problems with received packets if doing so would not help
	  much.

	  See Documentation/unaligned-memory-access.txt for more
	  information on the topic of unaligned memory accesses.

config ARCH_USE_BUILTIN_BSWAP
	bool
	help
	 Modern versions of GCC (since 4.4) have builtin functions
	 for handling byte-swapping. Using these, instead of the old
	 inline assembler that the architecture code provides in the
	 __arch_bswapXX() macros, allows the compiler to see what's
	 happening and offers more opportunity for optimisation. In
	 particular, the compiler will be able to combine the byteswap
	 with a nearby load or store and use load-and-swap or
	 store-and-swap instructions if the architecture has them. It
	 should almost *never* result in code which is worse than the
	 hand-coded assembler in <asm/swab.h>.  But just in case it
	 does, the use of the builtins is optional.

	 Any architecture with load-and-swap or store-and-swap
	 instructions should set this. And it shouldn't hurt to set it
	 on architectures that don't have such instructions.

config HAVE_ARCH_JUMP_LABEL
	bool

config HAVE_NMI
	bool

config HAVE_NMI_WATCHDOG
	depends on HAVE_NMI
	bool

config HAVE_CMPXCHG_LOCAL
	bool

config HAVE_CMPXCHG_DOUBLE
	bool

config HAVE_PERF_EVENTS_NMI
	bool
	help
	  System hardware can generate an NMI using the perf event
	  subsystem.  Also has support for calculating CPU cycle events
	  to determine how many clock cycles in a given period.

config HAVE_PERF_REGS
	bool
	help
	  Support selective register dumps for perf events. This includes
	  bit-mapping of each registers and a unique architecture id.

config HAVE_PERF_USER_STACK_DUMP
	bool
	help
	  Support user stack dumps for perf event samples. This needs
	  access to the user stack pointer which is not unified across
	  architectures.

config HAVE_CC_STACKPROTECTOR
	bool
	help
	  An arch should select this symbol if:
	  - its compiler supports the -fstack-protector option
	  - it has implemented a stack canary (e.g. __stack_chk_guard)

config CC_STACKPROTECTOR
	def_bool n
	help
	  Set when a stack-protector mode is enabled, so that the build
	  can enable kernel-side support for the GCC feature.

choice
	prompt "Stack Protector buffer overflow detection"
	depends on HAVE_CC_STACKPROTECTOR
	default CC_STACKPROTECTOR_NONE
	help
	  This option turns on the "stack-protector" GCC feature. This
	  feature puts, at the beginning of functions, a canary value on
	  the stack just before the return address, and validates
	  the value just before actually returning.  Stack based buffer
	  overflows (that need to overwrite this return address) now also
	  overwrite the canary, which gets detected and the attack is then
	  neutralized via a kernel panic.

config CC_STACKPROTECTOR_NONE
	bool "None"
	help
	  Disable "stack-protector" GCC feature.

config CC_STACKPROTECTOR_REGULAR
	bool "Regular"
	select CC_STACKPROTECTOR
	help
	  Functions will have the stack-protector canary logic added if they
	  have an 8-byte or larger character array on the stack.

	  This feature requires gcc version 4.2 or above, or a distribution
	  gcc with the feature backported ("-fstack-protector").

	  On an x86 "defconfig" build, this feature adds canary checks to
	  about 3% of all kernel functions, which increases kernel code size
	  by about 0.3%.

config CC_STACKPROTECTOR_STRONG
	bool "Strong"
	select CC_STACKPROTECTOR
	help
	  Functions will have the stack-protector canary logic added in any
	  of the following conditions:

	  - local variable's address used as part of the right hand side of an
	    assignment or function argument
	  - local variable is an array (or union containing an array),
	    regardless of array type or length
	  - uses register local variables

	  This feature requires gcc version 4.9 or above, or a distribution
	  gcc with the feature backported ("-fstack-protector-strong").

	  On an x86 "defconfig" build, this feature adds canary checks to
	  about 20% of all kernel functions, which increases the kernel code
	  size by about 2%.

endchoice

config HAVE_CONTEXT_TRACKING
	bool
	help
	  Provide kernel/user boundaries probes necessary for subsystems
	  that need it, such as userspace RCU extended quiescent state.
	  Syscalls need to be wrapped inside user_exit()-user_enter() through
	  the slow path using TIF_NOHZ flag. Exceptions handlers must be
	  wrapped as well. Irqs are already protected inside
	  rcu_irq_enter/rcu_irq_exit() but preemption or signal handling on
	  irq exit still need to be protected.

config DEBUG_SPINLOCK
	bool "Spinlock debug feature support"
	---help---
	  This enables support for debugging spinlocks. If you want to debug
	  spinlocks, say Y.

	  If you don't know what to do here, say N.

endmenu
